{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a108e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Galaxy Morphology Classification Project (Colab-Friendly)\n",
    "\n",
    "**Version 4: Better Training Parameters**\n",
    "This version\n",
    "- Increases training time (EPOCHS = 30)\n",
    "- Decreases the initial LEARNING_RATE (1e-4)\n",
    "- Decreases the FINE_TUNE_LR (1e-6)\n",
    "This should allow the model to learn properly.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 0. Configuration & Constants ---\n",
    "# PLEASE UPLOAD 'training_solutions_rev1.csv' and 'images_training_rev1.zip'\n",
    "# to the main Colab directory BEFORE running this script.\n",
    "\n",
    "print(\"--- Running Colab Setup ---\")\n",
    "\n",
    "# Define file paths\n",
    "# In Colab, the root directory is /content/\n",
    "CSV_UPLOAD_PATH = 'training_solutions_rev1.csv'\n",
    "ZIP_UPLOAD_PATH = 'images_training_rev1.zip'\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'training_solutions_rev1.csv')\n",
    "IMAGE_DIR = os.path.join(DATA_DIR, 'images_training_rev1')\n",
    "\n",
    "# Model and training parameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "# --- PARAMETER CHANGES FOR V4 ---\n",
    "EPOCHS = 30             # MORE PATIENCE: Was 10\n",
    "FINE_TUNE_EPOCHS = 15   # MORE PATIENCE: Was 5\n",
    "LEARNING_RATE = 1e-4    # MORE CAREFUL: Was 0.001\n",
    "FINE_TUNE_LR = 1e-6     # MORE CAREFUL: Was 1e-5\n",
    "# --- END OF CHANGES ---\n",
    "\n",
    "\n",
    "# --- 1. Colab File Setup ---\n",
    "# Create the 'data' directory if it doesn't exist\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "print(f\"Directory '{DATA_DIR}' ensured.\")\n",
    "\n",
    "# 1a. Move the CSV file\n",
    "if os.path.exists(CSV_UPLOAD_PATH):\n",
    "    # Only move if it's not already in the target location\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        os.rename(CSV_UPLOAD_PATH, CSV_PATH)\n",
    "        print(f\"Moved '{CSV_UPLOAD_PATH}' to '{CSV_PATH}'\")\n",
    "    else:\n",
    "        print(f\"'{CSV_UPLOAD_PATH}' already in root, but '{CSV_PATH}' also exists. Using existing.\")\n",
    "elif not os.path.exists(CSV_PATH):\n",
    "    print(f\"ERROR: '{CSV_UPLOAD_PATH}' not found in the root directory.\")\n",
    "    print(\"Please upload 'training_solutions_rev1.csv' and try again.\")\n",
    "    # Use exit() to stop the script if files are missing\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"CSV file already in place at '{CSV_PATH}'.\")\n",
    "\n",
    "\n",
    "# 1b. Unzip the images\n",
    "# We only unzip if the target directory doesn't already exist or is empty\n",
    "if os.path.exists(ZIP_UPLOAD_PATH):\n",
    "    if not os.path.exists(IMAGE_DIR) or not os.listdir(IMAGE_DIR):\n",
    "        print(f\"Unzipping '{ZIP_UPLOAD_PATH}' to '{DATA_DIR}'...\")\n",
    "        print(\"This will take several minutes. Please wait.\")\n",
    "        with zipfile.ZipFile(ZIP_UPLOAD_PATH, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DATA_DIR)\n",
    "        print(\"Unzipping complete!\")\n",
    "        # Optional: Remove the zip file after to save space\n",
    "        # os.remove(ZIP_UPLOAD_PATH)\n",
    "        print(f\"Successfully unzipped images to '{IMAGE_DIR}'\")\n",
    "    else:\n",
    "        print(f\"Image directory '{IMAGE_DIR}' already exists and is not empty. Skipping unzip.\")\n",
    "elif not os.path.exists(IMAGE_DIR) or not os.listdir(IMAGE_DIR):\n",
    "    print(f\"ERROR: '{ZIP_UPLOAD_PATH}' not found and '{IMAGE_DIR}' is also missing/empty.\")\n",
    "    print(\"Please upload 'images_training_rev1.zip' and try again.\")\n",
    "    exit()\n",
    "else:\n",
    "     print(f\"Image directory '{IMAGE_DIR}' already exists and is populated. Skipping unzip.\")\n",
    "\n",
    "print(\"--- Colab Setup Complete ---\")\n",
    "\n",
    "\n",
    "# --- 2. Load and Prepare Label Data ---\n",
    "\n",
    "print(\"\\nStep 2: Loading and preparing label data...\")\n",
    "\n",
    "# Load the CSV file\n",
    "try:\n",
    "    labels_df = pd.read_csv(CSV_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"CRITICAL ERROR: CSV file not found at {CSV_PATH} even after setup.\")\n",
    "    print(\"Please check the 'data' folder in the Colab file browser.\")\n",
    "    exit()\n",
    "\n",
    "# Format the 'GalaxyID' to match the image filenames\n",
    "labels_df['GalaxyID'] = labels_df['GalaxyID'].astype(str) + '.jpg'\n",
    "\n",
    "# Select the target columns\n",
    "target_columns = ['Class1.1', 'Class1.2', 'Class1.3']\n",
    "labels_df = labels_df[['GalaxyID'] + target_columns]\n",
    "\n",
    "print(f\"Loaded {len(labels_df)} labels.\")\n",
    "print(labels_df.head())\n",
    "\n",
    "\n",
    "# --- 3. Create File Paths and Split the Data ---\n",
    "\n",
    "print(\"\\nStep 3: Creating file paths and splitting data...\")\n",
    "\n",
    "print(\"Verifying image files exist...\")\n",
    "labels_df['filepath'] = labels_df['GalaxyID'].apply(\n",
    "    lambda x: os.path.join(IMAGE_DIR, x)\n",
    ")\n",
    "labels_df['file_exists'] = labels_df['filepath'].apply(lambda x: os.path.isfile(x))\n",
    "\n",
    "initial_count = len(labels_df)\n",
    "labels_df = labels_df[labels_df['file_exists']].copy()\n",
    "final_count = len(labels_df)\n",
    "\n",
    "if final_count == 0:\n",
    "    print(f\"ERROR: No image files were found in '{IMAGE_DIR}'.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Verified files: {final_count} images found (filtered out {initial_count - final_count} missing images).\")\n",
    "\n",
    "\n",
    "# Split the data\n",
    "train_df, val_df = train_test_split(\n",
    "    labels_df,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    val_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "\n",
    "# --- 4. Create tf.data Input Pipeline ---\n",
    "\n",
    "print(\"\\nStep 4: Building tf.data input pipeline...\")\n",
    "\n",
    "def load_and_preprocess_image(filepath, label):\n",
    "    \"\"\"\n",
    "    Loads and resizes the image.\n",
    "    CRITICAL FIX: EfficientNet expects pixels in range [0, 255].\n",
    "    We do NOT divide by 255.0 here.\n",
    "    \"\"\"\n",
    "    # Read the file from disk\n",
    "    img = tf.io.read_file(filepath)\n",
    "    # Decode as a 3-channel (RGB) JPEG\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Resize to the model's expected input size\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "\n",
    "    # --- DELETED THE DIVISION LINE ---\n",
    "    # img = img / 255.0  <-- This was the killer. It's gone now.\n",
    "\n",
    "    return img, label\n",
    "\n",
    "def create_dataset(df):\n",
    "    if df.empty:\n",
    "        return None\n",
    "    filepaths = df['filepath'].values\n",
    "    labels = df[target_columns].values\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
    "    ds = ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Create the training dataset\n",
    "train_ds = create_dataset(train_df)\n",
    "if train_ds:\n",
    "    train_ds = (\n",
    "        train_ds\n",
    "        .shuffle(buffer_size=SHUFFLE_BUFFER)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "# Create the validation dataset\n",
    "val_ds = create_dataset(val_df)\n",
    "if val_ds:\n",
    "    val_ds = (\n",
    "        val_ds\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "# Create the test dataset\n",
    "test_ds = create_dataset(test_df)\n",
    "if test_ds:\n",
    "    test_ds = (\n",
    "        test_ds\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "if not train_ds or not val_ds or not test_ds:\n",
    "    print(\"ERROR: One or more datasets are empty. Cannot proceed.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data pipelines built (train, validation, and test).\")\n",
    "\n",
    "\n",
    "# --- 5. Build the Model (Transfer Learning) ---\n",
    "\n",
    "print(\"\\nStep 5: Building the transfer learning model...\")\n",
    "\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Load the base model\n",
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    input_shape=IMG_SHAPE,\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build our new \"head\"\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=IMG_SHAPE),\n",
    "    # Add data augmentation layers\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax') # 3 output neurons\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# --- 6. Compile and Train the Model ---\n",
    "\n",
    "print(\"\\nStep 6: Compiling and starting initial training...\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model (only the new \"head\" layers)\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds\n",
    ")\n",
    "\n",
    "print(\"Initial training complete.\")\n",
    "\n",
    "\n",
    "# --- 7. Evaluate Initial Training ---\n",
    "\n",
    "print(\"\\nStep 7: Plotting initial training history...\")\n",
    "\n",
    "# Save plots\n",
    "try:\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig('initial_training_history.png')\n",
    "    print(\"Saved initial training plot to 'initial_training_history.png'\")\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error plotting history: {e}\")\n",
    "\n",
    "\n",
    "# --- 8. Fine-Tuning the Model ---\n",
    "\n",
    "print(\"\\nStep 8: Unfreezing base model for fine-tuning...\")\n",
    "\n",
    "base_model.trainable = True\n",
    "\n",
    "# Unfreeze the top 20 layers\n",
    "print(f\"Total layers in base model: {len(base_model.layers)}\")\n",
    "fine_tune_at = -20\n",
    "\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# We MUST re-compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=FINE_TUNE_LR),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "\n",
    "# Continue training from where we left off\n",
    "total_epochs = EPOCHS + FINE_TUNE_EPOCHS\n",
    "initial_epoch_num = EPOCHS\n",
    "\n",
    "fine_tune_history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=initial_epoch_num,\n",
    "    validation_data=val_ds\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning complete.\")\n",
    "\n",
    "\n",
    "# --- 9. Final Evaluation and Prediction ---\n",
    "\n",
    "print(\"\\nStep 9: Plotting combined training history and final evaluation...\")\n",
    "\n",
    "# Combine the initial and fine-tuning history\n",
    "try:\n",
    "    acc += fine_tune_history.history['accuracy']\n",
    "    val_acc += fine_tune_history.history['val_accuracy']\n",
    "    loss += fine_tune_history.history['loss']\n",
    "    val_loss += fine_tune_history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.ylim([min(plt.ylim()), 1])\n",
    "    plt.plot([EPOCHS-1, EPOCHS-1], plt.ylim(), label='Start Fine-Tuning', linestyle='--')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Combined Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.plot([EPOCHS-1, EPOCHS-1], plt.ylim(), label='Start Fine-Tuning', linestyle='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Combined Training and Validation Loss')\n",
    "    plt.savefig('combined_training_history.png')\n",
    "    print(\"Saved combined training plot to 'combined_training_history.png'\")\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error plotting combined history: {e}\")\n",
    "\n",
    "\n",
    "# Evaluate the model on the TEST set\n",
    "print(\"\\nEvaluating model on the test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Model Demonstration ---\")\n",
    "print(\"Running predictions on a batch of test data...\")\n",
    "\n",
    "# Get a batch of images and labels from the test set\n",
    "image_batch, label_batch = next(iter(test_ds))\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(image_batch)\n",
    "\n",
    "# Show the first 5 examples\n",
    "for i in range(5):\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(image_batch[i])\n",
    "\n",
    "    true_label = label_batch[i].numpy()\n",
    "    pred_label = predictions[i]\n",
    "\n",
    "    true_class_index = np.argmax(true_label)\n",
    "    pred_class_index = np.argmax(pred_label)\n",
    "\n",
    "    title = f\"True: {target_columns[true_class_index]} ({true_label[true_class_index]:.2f})\\n\"\n",
    "    title += f\"Pred: {target_columns[pred_class_index]} ({pred_label[pred_class_index]:.2f})\"\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'prediction_example_{i}.png')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  File: {test_df.iloc[i]['GalaxyID']}\")\n",
    "    print(f\"  True Label (Smooth, Disk, Star): {np.around(true_label, 2)}\")\n",
    "    print(f\"  Pred Label (Smooth, Disk, Star): {np.around(pred_label, 2)}\")\n",
    "\n",
    "print(\"\\n--- Project Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
